Abstract With the abundance of data and information in today’s time, it is nearly impossible for man, or, even machine, to go through all of the data line by line  What one usually does is to try to skim through the lines and retain the absolutely important information, that in a more formal term is called summarization  Text summarization is an important task that aims to compress lengthy documents or articles into shorter, coherent representations while preserving the core information and meaning  This project introduces an innovative approach to text summarization, leveraging the capabilities of Graph Neural Networks (GNNs) and Named Entity Recognition (NER) systems  GNNs, with their exceptional ability to capture and process the relational data inherent in textual information, are adept at understanding the complex structures within large documents  Meanwhile, NER systems contribute by identifying and emphasizing key entities, ensuring that the summarization process maintains a focus on the most critical aspects of the text  By integrating these two technologies, our method aims to enhances the efficiency of summarization and also tries to ensures a high degree relevance in the condensed content  This project, therefore, offers a promising direction for handling the everincreasing volume of textual data in an informationsaturated world  Index Terms—Graph Neural Networks, Named Entity Recognition, Text Summarization I  INTRODUCTION A  Background 1) Introduction to Text Summarization: Text summarization, in its essence, is the process of extracting the most important information from a text to produce a concise version that conveys the core meaning while reducing size of the text  This technique is divided into two primary categories: extractive summarization and abstractive summarization  Extractive summarization focuses on selecting and compiling key segments directly from the source text, whereas abstractive summarization involves generating new text that conveys the main ideas in a condensed form  Our focus lies on the former, exploring the nuances of extractive text summarization and its advancements  2) Relevance in Today’s World: In the current digital era, we are overwhelmed with textual information  From online news articles and scientific journals to social media feeds and corporate reports, the sheer volume of text available poses a significant challenge for individuals seeking to grasp information efficiently  Text summarization emerges as a potential and important solution in this context, enabling quick and effective access to essential content  Its applications span various domains where timely and accurate information extraction is quite important  3) Transition to GNN and NER in Text Summarization: While there have been various advancements in the field of text summarization, the incorporation of Graph Neural Networks (GNN) and Named Entity Recognition (NER) stands out  These technologies address the unique challenges posed by the volume and complexity of modern data  GNN, with its capacity to model relational data, and NER, effective in identifying and classifying named entities in text, together improve the extractive summarization process  This integration can not only improves the accuracy of summarization but also ensure relevance and contextawareness, a critical aspect when dealing with diverse and extensive datasets  B  Problem Statement In the current times, we are confronted with an evergrowing size of textual data of great variety  This information overload not only challenges our capacity for comprehension but also strains traditional text summarization methods  These conventional approaches often fall short in addressing the complexity and scale of the data, leading to inefficiencies and missed insights  Amidst this, Large Language Models (LLMs) have emerged as a powerful tool in text processing, showcasing an impressive ability to understand and generate humanlike text  However, their implementation has the downside of significant computational costs and resource demands, making them less feasible for continuous use in various resourceconstrained environments  This situation presents a critical need for more accessible and efficient text summarization solutions  In this context, the integration of Graph Neural Networks (GNN) and Named Entity Recognition (NER) holds immense potential for revolutionizing extractive text summarization  GNNs excel in modeling complex data relationships, while NER systems are proficient in identifying and categorizing key entities within texts  Individually, they have shown promise in various text processing applications, but their combined use in extractive text summarization remains an uncharted territory  This convergence presents a unique opportunity to address the limitations of current summarization methods, especially in managing large volumes of data efficiently and effectively  The core challenge lies in investigating the feasibility and effectiveness of combining GNN and NER in a unified framework  Such an integration aims not only to enhance the capacity for handling largescale data but also to provide a resourceefficient alternative to the more demanding LLMs  C  Objectives The primary objectives of this project are outlined as follows: 1) Explore the Integration of GNN and NER for Extractive Text Summarization: Implement the integration and test if in reality, the theoretical idea of the integration is practically effective once poised with the task of summariaztion  2) Create a ResourceEfficient Summarization Tool: Design a text summarization system that operates with significantly lower computational resources compared to Large Language Models (LLMs), making advanced text summarization more accessible in various environments  3) Evaluate the Performance of the GNNNER Framework: Conduct thorough testing and evaluation of the proposed GNNNER framework across diverse realworld scenarios, assessing accuracy and efficiency  4) Enhance ContextAwareness in Summarization: Focus on improving the contextawareness in the summarization process, ensuring the summarized content retains the essential context and meaning of the original text with the help of proposed NER integration  II  RELATED WORK The field of extractive text summarization has seen significant advancements in recent years, with researchers exploring various methodologies to enhance the efficiency and accuracy of summarization processes  1) GraphBased Approaches: [1]Kadriua and Obradovica (2021) explored an extractive text summarization method using graphs, which offers insights into the structural analysis of texts for summarization purposes  Further advancements in graphbased methods can be seen in other works, where the focus is on integrating graph structures with neural network models to capture contextual relationships in text  2) Named Entity Recognition in Summarization: [2]Alshibly et al  (2023) investigated text summarization of news articles using Named Entity Recognition (NER) via the SpaCy library, underscoring the importance of identifying key entities in text for effective summarization  This is complemented by studies which delve into the integration of NER with machine learning algorithms to enhance the precision of extractive summarization  3) LanguageIndependent Summarization Techniques: [3]HernándezCastañeda et al  (2022) discussed a languageindependent approach to extractive automatic text summarization, focusing on automatic keyword extraction, which is crucial for understanding multilingual and crosslingual summarization techniques  4) Algorithmic Innovations in Summarization: [4]P
Verma,
Verma, and Pal presented an approach using fuzzy evolutionary and clustering algorithms for extractive text summarization, highlighting the role of advanced computational methods in this domain  The evolution of algorithmic approaches can further be seen in various other studies, which explore the use of deep learning techniques for enhancing the adaptability and accuracy of summarization tools  Each of these studies contributes to a refined understanding of extractive text summarization  However, there remains a gap in exploring the combined potential of GNN and NER for this purpose  While individual studies have shown the efficacy of GNN in understanding complex relational data and NER in accurately identifying key entities, their integration in the context of extractive text summarization remains underexplored  This gap presents an opportunity for novel research, potentially leading to efficient summarization tools capable of handling the vast and varied nature of today’s textual data
This section outlines the methodology adopted for integrating Graph Neural Networks (GNN) and Named Entity Recognition (NER) in extractive text summarization
The methodology is structured into several key phases: 
The process is initiated with a document that undergoes a series of text preprocessing steps
These steps include tokenization, where the text is split into individual tokens, cleaning, to remove any irrelevant characters or spaces, normalization, which standardizes the text, and dependency parsing to analyze the grammatical structure of the text
Following preprocessing, the text is passed through the NER phase, where spaCy is used for entity tagging, identifying named entities within the text
This phase is critical for recognizing important nouns and noun phrases that are essential to understanding the context of the document
The extracted sentences and entities are then constructed into a graph, with sentences as nodes, entities also as nodes, and edges based on the relationships derived from the text and entity analysis
This graph is analyzed using GNN, specifically employing algorithms to evaluate the importance of each node within the graph structure
In the final phase of Summary Generation, sentences are selected based on their ranking from the graph analysis
Highranking sentences are deemed to carry significant informational weight and are chosen to be a part of the final summary
The summary is generated by strategically combining these sentences to reflect the main ideas of the original document while maintaining coherence and conciseness
This architecture is designed to optimize the summarization process by leveraging the strengths of both GNN for capturing relational information and NER for emphasizing key entities, resulting in summaries that are both informative and representative of the source document

Data Collection and Preprocessing The project utilizes the CNN Daily Mail dataset, renowned for its diverse compilation of news stories, ideal for tasks in natural language processing, including summarization
The selection criteria for textual data included relevance to current events, diversity in topics, and rich entity representation to facilitate effective NER
• Data Collection: The dataset is accessed through established repositories, ensuring that the data is reliable and can serve its purpose
The CNN Daily Mail dataset, can be seen from the statistics above, is a good choice for our project on extractive text summarization due to its substantial volume and diversity of length of articles as well as the diversity in the length of summaries provided
With over 300,000 combined articles from CNN and Daily Mail, the dataset offers a rich resource in terms of diversity and volume of data
The varying lengths of articles and summaries ensure that our system can adapt to a range of contexts and detail levels, which is an essential aspect for various usecases
Moreover, the extensive vocabulary present in the dataset is advantageous for developing a robust model capable of understanding and summarizing a wide array of topics
This variability and complexity make the CNN Daily Mail dataset an ideal candidate to serve its purpose for our research
•  PreProcessing: Data preprocessing forms the foundation of our text summarization model, as the subsequent graph representation relies heavily on the quality and richness of the input
The process comprises several stages, to refine the data for the construction of textrich nodes and their edges/relationships within the graph
The steps are as follows: – Tokenization: The text is broken down into tokens
This step is crucial as it determines the granularity of the nodes in our graph
Each token potentially represents a node implying it to require careful segmentation
– Cleaning (Punctuation/Special Characters): We remove punctuation and special characters that do not contribute to the semantic relationships in the graph
This step helps in reducing noise and focusing on the text that contributes to the meaning of the content
– Lemmatization: Lemmatization is applied to reduce words to their base or dictionary form
We aim to capture the essence of the text in the graph, this step helps in generalizing the connections between nodes by reducing the words to their base form
– Lower Casing: Converting all text to lower case ensures that the same words are not treated as different nodes due to case differences
– StopWord Removal: Common stopwords are filtered out as they usually do not carry significant meaning and are not useful as nodes in the graph
Their removal increases the textrichness of the remaining nodes, which bear more semantic weight
– Dependency Parsing: Perhaps the most critical step, dependency parsing, is employed to understand the grammatical structure of the text, which informs the edges in our graph
It elucidates the syntactic relationships between tokens, allowing for a richer and more meaningful representation of the text as a graph
Each of these preprocessing steps contributes to the construction of a graph where the nodes (words or entities) and their relationships (edges) are indicative of the content’s semantic structure
Given that our approach heavily relies on the accurate representation of textual relationships through graphs, the preprocessing stage is not merely a prerequisite but a determinant of the summarization quality
The more textrich and semantically accurate our graph, the better our model can identify and extract the most important/appropriate information for summarization
This attention to preprocessing detail ensures that our GNN can operate on data that closely mimics the interconnected nature of human language and thought

Implementation of Named Entity Recognition (NER) Named Entity Recognition (NER) is a critical stage in our text summarization process, going beyond basic preprocessing by enriching the text data with semantic information
This step acts as an additional input generation step
Output from the NER is given into the Graph Construction part of the pipeline
We utilize the spaCy library for its stateoftheart NER capabilities
An overview of this part of our pipeline can be seen below: The implementation of NER is a decisive step that, in theory, should significantly influence the performance of our GNN model
By accurately identifying and classifying entities, we create a detailed semantic map of the text, which is essential for generating summaries that are both informative and reflective of the text’s core themes

Incorporation of Graph Neural Networks (GNN) Graph Neural Networks (GNNs) are leveraged to capture the complex relational information among entities and contextual words in the text, effectively modeling the data as a graph
• NER Model Selection: We made use of spaCy’s advanced pretrained NER model, which identifies a wide range of entities with high precision
The model’s robustness is derived from its training on a diverse and extensive corpus, enabling it to recognize and tag entities accurately across various domains [2]
Our selection of spaCy’s model is based on its exceptional performance in benchmarks and its ease of integration into our proposed pipeline
• Entity Identification: The NER model tags text with entity types such as persons, organizations, locations, and others
This step is essential, as entities often carry significant weight in the meaning of a sentence and play a crucial role in summarization
By identifying these entities, we are able to preserve critical information that may otherwise be lost in a purely extractive approach
• Graph Construction with Entities: In the subsequent stage of our pipeline, the recognized entities become nodes within our graph
This is not a simple transformation of text into graph form; rather, it is a careful transformation of the text into a structured representation that highlights the most meaningful components
The entities serve as pivotal points in the graph, often becoming central nodes that connect with various parts of the text, reflecting their importance in the narrative structure
• Semantic Enrichment: The classification of entities facilitates a deeper understanding of the text by the GNN
It allows the GNN to not just process the text as a sequence of words but as a semantically rich network where the relationships between entities and other textual elements can be analyzed in a way that is closer to human cognition
GNN Model Selection: Our selected GNN architecture is specifically designed to process textbased graph structures
It is designed to understand and make use of the relational dynamics between words and entities, which are essential for identifying crucial information in the text summarization process
The architecture is chosen for its ability to handle the heterogeneity of the graph’s nodes and the complexity of their interrelations
• Graph Construction: The graph is constructed in a twostep process
Initially, sentences and identified entities are treated as separate nodes
These nodes are then interconnected through edges that represent their relationships, which are determined by cooccurrence within sentences, syntactical dependencies, and semantic relatedness 
This results in a rich graph where the nodes are text elements and the edges represent the contextual and relational nuances of the original document
• Edge Formation: Edges are not uniformly created; instead, their formation is informed by the depth of the relationship between nodes
Factors such as the frequency of cooccurrence and the strength of dependency links play a role in defining the edge weights, which are crucial for the GNN as it processes the graph to identify which nodes (text segments) are pivotal for the summary
The incorporation of GNNs enables the system to move beyond linear text analysis, instead treating the text as a complex network, somewhat similar to human cognitive processes that interpret information
This networked approach allows for a more detailed understanding and generation of summaries, which are both comprehensive and contextually coherent